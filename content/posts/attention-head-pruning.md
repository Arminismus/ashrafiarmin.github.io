+++
title = 'Attention Head Pruning'
date = 2024-02-02T10:40:16+03:30
draft = false
+++

The article [Are Sixteen heads really better than one?](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf) talks about the difference between having many or one attention heads, during training or test time. 

### Why would that matter?

One of the innovations of [The Transformer](https://arxiv.org/abs/1706.03762), was **Multi-Head Attention**.

